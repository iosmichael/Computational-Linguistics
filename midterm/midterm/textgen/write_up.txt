Text Generation Application:

The problem is to build a program that generates a paragraph that closely resemble the author's writing attribute. To best tackle this problem, I use the current three language models: unigram model, bigram model and trigram model, and combine them in a hierarchical way. The first word in the paragraph is generated by unigram as the most probable word that occur in the author text. The second word is generated with bigram distribution and the words after the second are all generated by trigram model. 

However, there are three problems that come with my initial approach:
1. Unwanted symbol like "PNCT","NUM","'" occurs frequently in the generated text. 
Fix: I simply remove those symbols from the word list that algorithm is generating paragraph from.
2. Occurance of zero probability with one model. It is very likely to have "word" guess that has zero or close to zero probability from trigram language model.
Fix: I use a hierarchical approach to solve this problem. If the algorithm is unable to come up with convincing candidate, it will downgrade its language model until it gives out promising word guess. [implementation see in code]
3. High probable words tend to form into a loop.
Fix: After solving the previous two problems, I find out that high probable guess tend to form a loop that makes the paragraph repeat the same phrase over and over again. In order to deal with this problem, I introduce a randomizer that randomly choose a word between the top two candidates. This will prevent algorithm running into a word loop and possibly provide interesting mutations to the paragraph it generates.
